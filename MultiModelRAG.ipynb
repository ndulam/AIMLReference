{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlACRW8FAjxlngbi/SY3H9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndulam/AIMLReference/blob/main/MultiModelRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Project Implementation: Multi-Modal RAG with OpenAI and Chroma DB\n",
        "Let's create a basic Multi-Modal RAG system using OpenAI's GPT-4 Vision model and Chroma DB for vector storage. This project will allow users to ask questions about images, retrieving relevant information from a database of image-text pairs.\n"
      ],
      "metadata": {
        "id": "erU8RPyTqUFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Set up the environment\n",
        "First, install the required libraries:\n",
        "bash"
      ],
      "metadata": {
        "id": "gz_FDHKeqZMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai chromadb pillow"
      ],
      "metadata": {
        "id": "AUgI0eWFqcIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Initialize the project\n",
        "Create a new Python file named multimodal_rag.py and add the following imports:"
      ],
      "metadata": {
        "id": "MfYFJ1WLqfzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import openai\n",
        "import chromadb\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"your_openai_api_key_here\""
      ],
      "metadata": {
        "id": "N5paVq4rss4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a function to encode images\n",
        "Add a function to encode images to base64:"
      ],
      "metadata": {
        "id": "MJKEGvpHsx0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')"
      ],
      "metadata": {
        "id": "GaEpQVTBszT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Set up Chroma DB\n",
        "Initialize Chroma DB and create a collection:"
      ],
      "metadata": {
        "id": "JN33gkxOs0Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\"image_text_pairs\")"
      ],
      "metadata": {
        "id": "wVZbHMDls5FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Add sample data to Chroma DB\n",
        "Add some sample image-text pairs to the database:"
      ],
      "metadata": {
        "id": "ewYdSPUxs9Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "    {\"image_path\": \"path/to/eiffel_tower.jpg\", \"description\": \"The Eiffel Tower in Paris, France\"},\n",
        "    {\"image_path\": \"path/to/statue_of_liberty.jpg\", \"description\": \"The Statue of Liberty in New York, USA\"},\n",
        "    # Add more samples as needed\n",
        "]\n",
        "\n",
        "for idx, item in enumerate(sample_data):\n",
        "    encoded_image = encode_image(item[\"image_path\"])\n",
        "    collection.add(\n",
        "        documents=[item[\"description\"]],\n",
        "        metadatas=[{\"image\": encoded_image}],\n",
        "        ids=[f\"img_{idx}\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "8mmGHKlFs-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Implement the Multi-Modal RAG query function\n",
        "Create a function to process user queries:"
      ],
      "metadata": {
        "id": "40jYZHJFtCee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_rag_query(query, image_path):\n",
        "    # Encode the query image\n",
        "    encoded_query_image = encode_image(image_path)\n",
        "\n",
        "    # Retrieve relevant information from Chroma DB\n",
        "    results = collection.query(query_texts=[query], n_results=1)\n",
        "\n",
        "    if results[\"documents\"]:\n",
        "        context = results[\"documents\"][0][0]\n",
        "        context_image = results[\"metadatas\"][0][0][\"image\"]\n",
        "    else:\n",
        "        context = \"No relevant information found.\"\n",
        "        context_image = None\n",
        "\n",
        "    # Prepare the messages for GPT-4 Vision\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that can see and analyze images.\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": f\"Query: {query}\\n\\nContext: {context}\"},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_query_image}\"}}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    if context_image:\n",
        "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{context_image}\"}})\n",
        "\n",
        "    # Generate a response using GPT-4 Vision\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-vision-preview\",\n",
        "        messages=messages,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "Q9DFxHzBtDQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Test the Multi-Modal RAG system\n",
        "Add a main section to test the system:"
      ],
      "metadata": {
        "id": "tKFs5wHxtIG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"What can you tell me about this landmark?\"\n",
        "    image_path = \"path/to/query_image.jpg\"\n",
        "\n",
        "    response = multimodal_rag_query(query, image_path)\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "KFzN31pbtJK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation creates a simple Multi-Modal RAG system that can answer questions about images by retrieving relevant information from a database of image-text pairs. The system uses OpenAI's GPT-4 Vision model to generate responses based on the query image and the retrieved context.\n",
        "To use this system, ensure you have valid image paths and an OpenAI API key. You can expand the sample data and fine-tune the retrieval process to improve the system's performance for your specific use case.\n",
        "By leveraging Multi-Modal RAG, software engineers can create more powerful and versatile AI applications that can understand and process various types of data, leading to more engaging and informative user experiences."
      ],
      "metadata": {
        "id": "5nllFPbItLv6"
      }
    }
  ]
}